
\chapter{SpartanRPC and Sprocket}
\label{chapter-spartanrpc-sprocket}

\lstset{language=nesC}
\lstMakeShortInline[basicstyle=\ttfamily]!

In this chapter I describe SpartanRPC \cite{chapin-skalka-SpartanRPC,chapin-skalka-SpartanRPCTR}
and its implementation in the \Sprocket\ compiler \cite{sprocket}.

SpartanRPC is a new programming language based on nesC that provides built-in support for
authorized remote procedure calls. SpartanRPC as a language allows potentially many different
forms of access control to be used, however \Sprocket\ currently only supports the use of the
$RT_0$ trust management system. \Sprocket\ also uses radio links to implement \newterm{dynamic
  wires} (as described in \autoref{section-dynamic-wires}) and thus targets TinyOS and wireless
sensor networks. However, there is nothing in the design of SpartanRPC that would preclude the
use of other communications channels.

While there has been a great deal of research on security in sensor networks much of that work
has focused on low level concerns such as link layer security, key distribution
\cite{camtepe-bulent-05}, and secure network protocols \cite{1049776,fouladgar-3tls-2006}.
Systems such as TinySec \cite{karlog-tinysec-2004} and MiniSec \cite{luk-minisec-2007} are based
on shared secrets and generally assume that an entire network comprises a single security
domain. Furthermore, these systems support confidentiality and integrity properties, but not
access control.

In contrast the use of a trust management system allows embedded developers to specify high
level authorization policies that permit different security domains to interact without prior
introduction. In a sensor network context this might arise when, for example, the wearer of a
body area network enters a region of space covered by a metropolitan network. These networks may
have never encountered each other and yet wish to access sensitive functions, such as for
medical monitoring and control.

Trust management systems use public key cryptography and require some mechanism for evaluating
authorization requests in the light of an access policy (the $L$ in \autoref{figure-tmstruct}).
Although the feasibility of using public key cryptography on sensor nodes has been shown by
several authors
\cite{1049776,Malan:2008:IPI:1387663.1387668,bertoni-2006,kumar-2006,4604657,Liu-Peng-TinyECC-2008,Szczechowiak:2008:NTL:1786014.1786040},
combining this with the necessary authorization decision to support a full trust management
system, and showing the feasibility and practicality of doing so on resource constrained
devices, has not been previously demonstrated. As I will show in \autoref{chapter-evaluation}
the \Sprocket\ runtime system exacts a significant performance penalty on the nodes,
particularly with respect to system startup time. Yet despite this problem useful work can still
be accomplished.

\section{Overview and Applications}
\label{section-overview}

The SpartanRPC language allows network administrators to define $RT_0$ policies that mediate
access to specified resources on network nodes. A \emph{resource} in SpartanRPC is user-defined
functionality programmed in an extension of nesC, and accessible in RPC style by client code
programmed in the same extension of nesC. Thus, while previous systems have explored the problem
of establishing multiple security domains in a wireless sensor network
\cite{Claycomb:2011:NNL:1889383.1889450}, and others have considered RPCs in sensor networks
\cite{may-tinyrpc-2007,bergstrom-anycastrpc-2007,5766863}, SpartanRPC provides a
readily-accessible mechanism that combines these features. Furthermore, SpartanRPC's use of
$RT_0$ allows specification of fine-grained, decentralized security policies.

Since SpartanRPC is flexible and easily accessible to TinyOS programmers, these features can be
used with a variety of applications. Consider a first-responder situation, in which multiple
social entities must interact and cooperate. Recent work has shown the effectiveness of wireless
sensor networks in such scenarios \cite{citeulike:4460555,1038146}, in their ability to
coordinate multiple data collection and communication devices in an ad-hoc, easily deployable
manner. However, data collection and communication in this scenario (and other similar ones)
must be a secured resource, due to e.g.~HIPA requirements in the case of medical response.
Furthermore, security must be coordinated on-site in a sensor network comprising subnetworks
administered separately (police, medical units from different hospitals, etc.), and no prior
coordination between administrations can generally be assumed. The SpartanRPC system is designed
to address these types of scenarios.

For example, if an EMT team deploys a sensor network to monitor patient locations and vital
signs, a security policy can be imposed whereby responding police departments can deploy their
own sensor network, and through it access patient identity and location data but \emph{not}
medical data directly from the EMT network. This direct data access will often be necessary due
to real-time constraints and lack of Internet connectivity in emergency situations. Thus the
direct approach to managing access described in this chapter would be essential; a staged
approach may not have sufficiently fast turn-around time.

Time synchronization is another important sensor network function that is security sensitive,
since many higher-level protocols rely on it. A number of previous authors have considered
secure time synchronization in the presence of ``insider'' attacks
\cite{Manzo:2005:TSA:1102219.1102238,Ganeriwal:2008:STS:1380564.1380571}, whereby nodes within
the network may be compromised and function as malicious actors capable of corrupting the
protocol. In particular, the FTSP protocol can be attacked by a single compromised ``root'' node
injecting false timing information into the network \cite{Manzo:2005:TSA:1102219.1102238}, even
when symmetric keys are used for secure information exchange. However, the threat model in this
work treats all nodes in a network as equally compromisable. In cases where a connected
sub-component of a network running an FTSP protocol is more resistant to compromise, due to,
e.g., differences in hardware, a SpartanRPC policy can be established whereby only nodes in the
most tamper-resistant sub-component of the network may function as roots. FTSP time sync updates
on any given node can be defined to require a root authorization level. This implies that nodes
requiring secure time synchronization must be at most a single radio hop from a root node, but
nodes willing to accept possibly corrupted time sync data can extend the network indefinitely.
Note that in this scenario, SpartanRPC policies adapt to heterogeneity in network device
hardware, vs.~network administration as in the previous example.

Other potential applications of this system include secure routing protocols in heterogeneous
trust environments \cite{senroute-ahnj03}, transport and network layer protocols
\cite{perillo-heinzelman-2005}, tracking protocols \cite{brooks-ramanathan-sayeed-2003}, and
even node-based web servers supporting secure channels \cite{1049776}.

\section{Technical Foundations}

\paragraph{Language-Based Security} SpartanRPC provides language-level abstractions for defining
remote services and associated security policies. Programmers are presented with an extension of
nesC, with new features for defining remote access controlled services, and for invoking those
services securely. This hides the implementation details of the underlying security protocols
and only requires mastery of $RT_0$, a simple authorization logic. SpartanRPC programs are
compiled in the same manner as nesC programs, in fact the SpartanRPC compiler rewrites
SpartanRPC programs to ordinary nesC code.

\paragraph{Asynchronous Remote Procedure Call} As other authors have observed
\cite{may-tinyrpc-2007}, RPC is an appropriate abstraction for node services on the network and
supports whole-network (vs.~node-specific) programming. Secure RPC is well-studied in a
traditional networking environment, and is a natural means of layering security over a
distributed communication abstraction.

It is necessary for RPC invocation in a wireless sensor network to be asynchronous, since
synchronous call-and-return to a remote node would significantly impede performance in the best
case and cause deadlock in the worst. In order to minimally impact the nesC programming model,
SpartanRPC defines RPC invocation as a form of \emph{remote task}. Local tasks are units of
programmer-defined asynchronous computation in nesC, so treating remote computational services
as remote tasks fits this paradigm. Remote tasks can be invoked on one-hop neighbors, providing
a link layer service on which network layer services can be built.

\paragraph{PK-Based Authorization Policies} SpartanRPC provides language-level abstractions for
specifying RPC authorization policies. The $RT_0$ trust management system allows network
entities to communicate credentials for authorizing service invocations. In SpartanRPC these
credentials are implemented with ECC public keys \cite{bertoni-2006}, which are validated during
the initial authorization phase. ECC is significantly more tractable than RSA in a resource
constrained setting. Furthermore, following an initial authorization phase my protocol
establishes a shared AES key for subsequent invocations of a given service by the same node.
Since hardware AES is available on common radio chipsets, I obtain highly efficient performance
for secure invocations following authorization. This is demonstrated with empirical results
reported in \autoref{chapter-evaluation}.

\section{Duties and Remotability}
\label{section-duties}

Because of the slow, unreliable nature of wireless communications it is unrealistic for RPC
services in wireless sensor networks to be synchronous. Instead I believe that the semantics of
tasks are a more appropriate abstraction. They are not quite right however, as RPC services will
typically require arguments to be passed---a feature not provided by nesC tasks---and while the
poster of a task defines it, an RPC service invokes remotely defined functionality. SpartanRPC
therefore defines a new RPC abstraction called a \emph{duty}.

\subsection{Syntax and Semantics}
\label{section-duties-syntax}

Duties are declared in nesC interfaces and syntactically resemble nesC command declarations.
Instead of using the reserved word !command! the new reserved word !duty! is used. Duties are
allowed to take parameters (with restrictions as discussed below) but must return the type
!void!. For example the following interface describes an RPC service for remotely controlling a
collection of LEDs:

\singlespace
\begin{lstlisting}
interface LEDControl {
    duty void setLeds( uint8_t ctl );
}
\end{lstlisting}
\primaryspacing

Duties are defined in modules in a manner similar to the way tasks, commands, or events are
defined. The reserved word !duty! is again used on the definition. Like commands and events the
name of the duty is qualified by the name of the interface in which it is declared. Including a
duty in an interface definition automatically implies that the interface can be remotely
invoked, or is \emph{remotable} in the sense formalized in \autoref{section-remotable}. Any
remotable interface provided by a component must be specified as !remote! in its provides
specification. The first code sample in \autoref{figure-duty-usage} shows an !LEDControllerC!
component that provides the !LEDControl! interface remotely i.e.~that allows remote nodes to
control LED status lights on a board.

\begin{fpfig}[t]{Duty Implementation and Invocation Examples}{figure-duty-usage}
{
\singlespace
\begin{lstlisting}
module LEDControllerC {
  provides remote interface LEDControl;
}
implementation {
  duty void LEDControl.setLeds(uint8_t ctl) { ... }
} 
 
module LoggerC {
  uses interface LEDControl;
}
implementation {
  void f() { ... post LEDControl.setLeds(42); }
}
\end{lstlisting}
\primaryspacing
}
\end{fpfig}

A module on the client node that wishes to use a remote interface simply posts the duty in the
same manner as tasks are posted. The use of !post! emphasizes the asynchronous nature of the
invocation. An example duty posting is illustrated in \autoref{figure-duty-usage}. The standard
component semantics of nesC provide a natural abstraction of ``where'' the RPC call goes, just
as a normal command invocation will go through a component interface that is disconnected from
its implementation. Like a normal command invocation, configuration wirings determine where duty
control flows. However, in SpartanRPC duty invocation flows to a component residing on a
different node. The invoking module must be connected to the remote modules by way of a dynamic
wire as described in \autoref{section-dynamic-wires}.

When a duty is posted by a client it may run at some time in the future on the server node. The
client node continues at once without waiting for the duty to start, i.e., duty postings are
asynchronous in the same manner that tasks are. Once posted the client has no direct way to
determine the status of the duty. Also, due to the unreliability of the network a posted duty
may not run at all. The success or failure of a duty posting is not signaled to the client in
the implementation just as, for example, the receipt or non-receipt of a message send is not
signaled in the \texttt{AMSend} protocol in TinyOS. Thus any error semantics for duty postings
must be implemented by the application developer.

\subsection{Remotable Interfaces}
\label{section-remotable}

SpartanRPC imposes certain requirements on RPC service definitions for ease of implementation.
First, since sensor network nodes do not share state passing nesC pointers to duties is
disallowed---such a reference would be meaningless on the receiving node. Thus remotable types
are defined as follows:
\begin{definition}
  A type is \emph{remotable} if and only if it satisfies the following inductive definition: The
  nesC built-in arithmetic types, including enumeration types, are remotable, and structures
  containing remotable types are remotable.
\end{definition}
Since a remotable interface describes RPC services, such interfaces are required to declare
duties taking only arguments of remotable type; also, remotable interfaces can only contain
duties, to ensure meaningful remote usage.
\begin{definition}
  An interface is \emph{remotable} if and only if it only provides duties whose argument types
  are remotable.
\end{definition}

\section{Dynamic Wires}
\label{section-dynamic-wires}

In an ordinary nesC program the ``wiring'' between components as defined by configurations is
entirely static. The nesC compiler arranges for all connections and at run time the code invoked
by each called command or signaled event is predetermined.

In a remote procedure call system for distributed embedded environments, especially those
communicating using radio links, this static arrangement is insufficient. A node can not, in
general, know its neighbors at compilation time but rather must discover this information after
deployment. In addition, the volatility of wireless links, and of the nodes themselves, means
that a given node's set of neighbors will change over time. In this section I discuss the
facility in SpartanRPC to allow \emph{dynamic wirings} for control flow from duty invocation via
remotable interfaces to duty implementation, wherein the programmer has control over wiring
endpoints and how they may change during program execution.

\subsection{Component IDs, Component Managers}
\label{section-componentmanager}

I begin by discussing how remote components are identified for wiring. In order to uniquely
identify components on a network of devices, remotable components are specified via a
two-element structure called a !component_id! defined on the left side of
\autoref{figure-componentmanager}. The !node_id! member is the same node ID used by TinyOS and
is set when the node is programmed during deployment. The local ID member is an arbitrary value
defined by the programmer of the server node. Only components that are visible remotely need to
have ID values assigned, however, the ID values must be unique \emph{on the node}. The
!component_set! structure defined on the right side of \autoref{figure-componentmanager} wraps
an arbitrary array of !component_id! values.
 
\begin{fpfig}[t]{Component Manager Interface and Type Definitions}{figure-componentmanager}
{
\begin{minipage}[t]{2.5in}
\singlespace
\begin{lstlisting}
typedef struct {
  uint16_t node_id;
  uint8_t  local_id;
} component_id;
\end{lstlisting}
\primaryspacing
\end{minipage}
\hfill
\begin{minipage}[t]{2.5in}
\singlespace
\begin{lstlisting}
typedef struct {
  int count;
  component_id *ids;
} component_set;
\end{lstlisting}
\primaryspacing
\end{minipage}
\\
\centering
\begin{minipage}[t]{5in}
\vspace{1.5em}
\singlespace
\begin{lstlisting}
interface ComponentManager {
  command component_set elements();
}
\end{lstlisting}
\primaryspacing
\end{minipage}
}
\end{fpfig}

A \emph{component manager} is a component that provides the !ComponentManager! interface defined
at the bottom of \autoref{figure-componentmanager}. It dynamically specifies a set of component
IDs that ultimately serve as dynamic wiring endpoints.

As a simple example, consider the component manager !RemoteSelectorC! as shown in
\autoref{figure-example-componentmanager}. This example component manager always returns a
component set containing a single component. However in general multiple components on
neighboring (one-hop) nodes could be selected. Thus dynamic wires are inherently a multi-cast
communication channel. In a more complex example the component manager would compute the
component set each time the dynamic wire is used, filling in an array of component IDs based on
information gathered earlier in the node's lifetime.

\begin{fpfig}[t]{Example Component Manager}{figure-example-componentmanager}
{
\singlespace
\begin{lstlisting}
module RemoteSelectorC {
  provides interface ComponentManager;
}
implementation {
  // 0xFFFF is the special broadcast address.
  // Local component #1 on each node selected.
  component_id  broadcast  = { 0xFFFF, 1 };
  component_set broadcast_set = { 1, &broadcast };

  command component_set ComponentManager.elements() {
    return broadcast_set;
  }
}
\end{lstlisting}
\primaryspacing
}
\end{fpfig}

\subsection{Syntax and Semantics}
\label{section-wiringsyntax}

In SpartanRPC the syntax and semantics of nesC is extended to allow the target of a connection
to be dynamically specified by a component manager. The syntax of wirings, or connections, is
extended as follows:

\singlespace
\begin{Verbatim}
        connection ::= endpoint '->' dynamic_endpoint
  dynamic_endpoint ::= '[' IDENTIFIER ']' ('.' IDENTIFIER)
\end{Verbatim}
\primaryspacing

Given a dynamic wiring of the form !C.I -> [RC].I!, I informally summarize its semantics as
follows. First, !RC! is statically required to be a component manager, and !I! must be
remotable. At run time, if control flows across this wire via posting of some duty !I.d! within
!C!, the command !elements! in !RC! is called to obtain a set of component IDs. The duties !I.d!
provided by those remote components will then be posted on the host nodes via an underlying link
layer communication, the details of which are hidden from the SpartanRPC programmer. Thus,
duties can only be posted on neighbors. Note that since this call to !elements! may return more
than one component ID, this is a sort of fan-out wiring.

For example, the programmer could wire the !LoggerC! component mentioned in
\autoref{figure-duty-usage} to LED controller components on a dynamically changing subset of
neighbors using a configuration such as:
\begin{lstlisting}
LoggerC.LEDControl -> [RemoteSelectorC].LEDControl;
\end{lstlisting}

The server's configuration does not need to wire anything to the remote interface explicitly.

\subsection{Callbacks and First-Class IDs}

I assume that the component IDs for well known services will be agreed upon ahead of time by a
social process outside of SpartanRPC. By broadcasting to a ``well known'' component ID, a node
can use services on neighboring nodes without knowing their node IDs. The use of well known ID
values is analogous to the use of well known TCP port numbers to provide easily accessible
Internet services.

If a node expects a reply from a service that it invokes, the invoking node must set up a
component with a suitable remote interface to receive the service's result. In SpartanRPC remote
invocations can only transmit information in one direction. Bidirectional data flow requires
separate dynamic wires. This design provides a natural ``split-phase'' semantic wherein the
invoker of a service can continue executing while waiting for the result of that service, a
common idiom for nesC programming. For example, a service might require the client to provide
the node ID and component ID of the component that will receive the service result as arguments
to the service invocation. The server could store those values for use by a server-side
component manager. It is permitted for a component to be its own component manager making it
easy for a service to return a result by posting the appropriate duty.

For example, assume that the LED controller on the server returns the old state of the LEDs
whenever the LED value is changed. The server configuration would include an appropriate
dynamic wire as follows
\begin{lstlisting}
LEDControllerC.LEDResult -> [LEDControllerC].LEDResult;
\end{lstlisting}

The client must provide the LEDResult interface remotely to receive this result. In this example
the !LEDControllerC! component is its own component manager. This makes it easy for the
!elements! command to access global data that was recorded inside !LEDControllerC! when the
service it provides was previously invoked. This is a common SpartanRPC idiom.

\section{Security Policy Specification}
\label{section-security-extensions}

In this section I discuss how to extend the language setting described previously with security
features. The goal is a language framework where RPC services require authorization for use, and
where authorization policies support collaboration between multiple social domains. The
authorization model can be viewed as a client-server interaction; respective sides of the
interaction protocol are summarized separately as follows.

\subsection{RPC Server Side Logic}
\label{section-rpc-server-side}

RPC service providers establish policy by assigning governing roles $A.g$ to remote interface
implementations. Service providers also possess a set of assumed credentials $\creds$, which
establish an authorization environment including, perhaps (but not necessarily), the access
policy. As I will describe in detail, the set $\creds$ may grow as additional credentials are
communicated to servers. Finally, in the presence of security, client invocations of any RPC
service are not anonymous, but are performed on behalf of some entity $B$, which must be a
member of the governing role $A.g$ to use the protected service.

In summary, access to an RPC level is allowed if and only if the property $\creds \vdash B \in
A.g$ holds, where:
\begin{itemize}
  \item $B$ is the identity of the RPC client.
  \item $A.g$ is the governing role of the RPC service.
  \item $\creds$ are the credentials known to the RPC server.
\end{itemize}
RPC service programmers specify governing roles as part of module definitions---specifically at
remote interface !provides! clauses. Hence, governing roles are associated with interface
\emph{implementations}, not interfaces themselves. This allows application flexibility, in that
the same interface can be implemented with various authorization levels within the same network.
Syntax is as follows:
\begin{lstlisting}[mathescape=true]
provides remote interface $I$ requires $A.g$
\end{lstlisting}

Note the minor modification to previously introduced syntax for remote module definitions, via
the !requires! keyword.

\subsection{RPC Client Side Logic}
\label{section-rpc-client-side}

In order to use a secure remote module, RPC clients wire to it as for unsecured modules (see
\autoref{section-wiringsyntax}), but with two additional capabilities: (1) the client specifies
under what \RT\ entity the invocations will be performed, and (2) the client may also specify
credentials in their possession which are to be activated for use in the invocation. Syntax is
as follows:
\begin{lstlisting}[mathescape=true]
activate "$C_1, \ldots, C_n$" as "$B$" for C.I -> [M].J
\end{lstlisting}
For any invocation made through this wiring the credentials $C_1, \ldots, C_n$ will be remotely
added to the RPC server's authorization environment for the authorization decision, via a
process detailed in \autoref{section-implementation}. Note that these credentials \emph{need not
  establish authorization entirely by themselves}, rather they will be \emph{added} to the
server's existing credentials, all of which will be used in the authorization decision. A
special form of the !enable! clause using !"*"! for the list of credentials is also supported.
This form indicates that all credentials known to the client should be communicated to the
server.

Each node is deployed with a collection of ECC key pairs, one for each entity the node
represents. When an invocation is made the entity $B$ mentioned in the !as! clause of the
dynamic wire is used in the request. The !as! clause is optional; if it is omitted a
distinguished \newterm{default entity} is used for the invocation.

\subsection{Example}
\label{section-security-example}

Suppose that an existing network deployment !NetA! is imaged with a component !SamplingRateC!
which provides a means to control sampling rates through an interface !SamplingRate!. Further,
since sampling rate modification is a sensitive operation, the network administrators require
!NetA.control! authorization to use this component:

\singlespace
\begin{lstlisting}
module SamplingRateC {
    provides remote interface SamplingRate requires "NetA.control";
}
\end{lstlisting}
\primaryspacing

Any node supporting this component will transparently receive \RT\ credentials from neighboring
nodes and attempt to use those credentials to establish that each client entity is a member of
the !NetA.control! role in the formal sense described above.

Suppose also that nodes in !NetA! are deployed with the credential
\begin{displaymath}
\code{NetA.control} \leftarrow \code{WSNAdmin.control}
\end{displaymath}
Here the role !WSNAdmin.control! is administered by some overarching network authority. However
this authority need not be physically ``present'' in the network during operation. Instead the
credential above represents !NetA!'s access control policy: any entity blessed by !WSNAdmin! as
a controller can control !NetA!.

Suppose further that another subnet, called !NetB!, wishes to modify the sampling rate of
!NetA!. A node in !NetB! might be imaged with the following credentials, among possibly others:

\singlespace
\begin{eqnarray}
\code{WSNAdmin.control} & \leftarrow & \code{NetB.control} \\
\code{NetB.control}     & \leftarrow & \code{NetB}
\end{eqnarray}
\primaryspacing

Note that credential (1) is issued by the !WSNAdmin! authority, while credential (2) is issued
by !NetB!. Critically, direct communication with !NetA! authorities to obtain these credentials
is unnecessary.

In order to invoke this service the wiring as shown in \autoref{figure-secure-wire} could be
made on the client side. Note the activation of the necessary credentials, as well as the
specification of client identity as !NetB!.

\begin{fpfig}[t]{Security Enabled Dynamic Wire}{figure-secure-wire}
{
\singlespace
\begin{lstlisting}
activate
  "WSNAdmin.control <- NetB.control, 
   NetB.control     <- NetB" as "NetB"
for 
  ClientC.SamplingRate -> [RemoteSelectorC].SamplingRate;
\end{lstlisting}
\primaryspacing
}
\end{fpfig}

\section{The SpartanRPC Implementation}
\label{section-implementation}

In this section I describe the \Sprocket\ implementation of the SpartanRPC system using $RT_0$
trust management for authorization. \Sprocket\ rewrites a SpartanRPC program into a pure nesC
program and provides a supporting runtime system. Program rewriting converts remote duty
postings into a nesC messaging protocol. The main task of the runtime system is to implement the
encapsulated, underlying security protocols for authorization of remote duty postings.

\subsection{Authorization and Security Protocols}
\label{section-security-protocols}
\label{section-underlying-protocols}

\Sprocket\ implements SpartanRPC authorization using a combination of public and symmetric key
cryptography. I used the TinyECC library \cite{Liu-Peng-TinyECC-2008} for public key
functionality, and AES encryption for symmetric key functionality. TinyECC uses elliptic curve
cryptography for more efficient public key operations in sensor networks. Using AES has the
benefit of hardware support on many current embedded platforms, e.g.~those employing the Chipcon
CC2420 radio.

There are three security protocols for authorized duty postings, illustrated in
\autoref{figure-sprocketrt-protocol}, that operate asynchronously. First, a credential exchange
protocol, wherein \RT\ credentials are communicated between nodes and authorization for various
entities are computed, i.e.~the \emph{minimum model} as described in
\autoref{section-security-extensions}. Second, a session key negotiation protocol, where
symmetric keys for multiple authorized service invocations between a duty client and server are
computed. And third, an authorized service invocation protocol, wherein duty posting requests
are checked to ensure the appropriate authorizations. This decomposition of authorized service
invocation into three protocols supports efficiency especially through the use of symmetric keys
for multiple service invocations. Its asynchronous nature is also appropriate in an asynchronous
TinyOS setting.

\begin{figure}[t]
  \input{Figures/SprocketRT-Protocol}
  \centerline{\raise 1em\box\graph}
  \vspace{2mm}
  \caption{SpartanRPC Security Protocol Elements}
  \label{figure-sprocketrt-protocol}
\end{figure}

\subsubsection{Credential Exchange}
\label{section-certificate-format}

SpartanRPC credentials are implemented as signed certificates. All SpartanRPC-enabled nodes
contains a certificate sender component and a certificate receiver component, to transfer
certificates between nodes and to verify them and interpret the credentials they represent. Both
components run as background daemons, performing their function asynchronously. A
SpartanRPC-enabled node is deployed with a collection of certificates in read-only storage
representing that node's credentials, which are determined by some external means. Once the node
is booted, the certificate sender starts a periodic timer. When the timer fires, the node
link-layer broadcasts (i.e.~only to neighbors) all certificates in its certificate storage that
are mentioned in the !enable! clauses in its program. To prevent adjacent node certificate
broadcasts from colliding, the certificate broadcast interval is modulated randomly by $\pm
10$\%. For example if the nominal broadcast interval is one minute, the actual time varies
randomly between 54 and 66 seconds.

The certificate distribution strategy is robust in the face of new nodes being added to the
network or intermittent radio connectivity. If a node fails to receive certain certificates from
its neighbors it will have another opportunity to do so when those neighbors rebroadcast their
certificates. There is a trade off between the broadcast interval, responsiveness, and network
energy consumption. A short broadcast interval allows authorizations to succeed ``quickly''
since neighbors become aware of the necessary credentials early, but at the cost of increased
radio traffic and power consumption.

Once a newly received certificate has been verified, the credential it represents is extracted
and stored. The credential storage also contains the $RT_0$ minimum model implied by the
currently known collection of credentials. Each time a new credential is added to storage, the
minimum model is updated. This is done by repeatedly applying authorization logic inference
rules implied by the credentials to the current model until a fixed point is reached, i.e.~a
logical closure \cite{Li:DCFTML}. Thus, each node maintains a local view of authorization levels
for network entities based on received credentials.

The certificate representation of an \RT\ credential contains the public keys denoting entities
mentioned in the credential. Roles are identified by one byte numeric codes and are scoped by
the entity defining the role. Credential forms are distinguished by numbers $\{ 1, 2, 3, 4 \}$.
Certificates are also signed by their issuing authority. Conveniently, the issuing authority is
always mentioned in a credential (e.g.~the issuing authority of $\cred{A.r}{B}{}$ is $A$) so the
public key required to verify the certificate is always included with it for free. This does not
introduce a security problem. Since entities are identified directly by their keys, an attacker
who creates a new key is simply creating a new entity.

The over-the-air format for the intersection certificate is illustrated in
\autoref{figure-certformats}. The other certificate forms are organized in a similar way.
Certificates range in size from 124 bytes for the membership credential to 166 bytes for the
intersection credential. This is larger than the maximum payload size limit of TinyOS T-Frame
Active Message packets as transported by IEEE 802.15.4 \cite{802.15.4,hui-tep125}. It is much
larger than the default maximum payload size of 28 bytes used by TinyOS \cite{levis-tep111}.
Consequently the certificates are fragmented into four messages requiring a maximum payload size
of 43 bytes. Notice that SpartanRPC limits intersection roles to just two subroles and does not
allow an arbitrary number of subroles as described in \autoref{section-rt}. This does not limit
expressivity because intermediate roles can be defined if necessary.

\begin{figure}[t]
  \input{Figures/Certificate-Formats}
  \centerline{\raise 1em\box\graph}
  \vspace{2mm}
  \caption{Intersection Certificate Format (parenthesized numbers indicate byte counts)}
  \label{figure-certformats}
\end{figure}

Message fragments are sent back to back with a 200 ms delay between each to allow the receiver
time to assemble them. Fragments are sent in order with no fragment identifiers. To stay
synchronized with the sender, receivers expect to receive all the fragments in a timely manner.
If a fragment is not received within 750 ms of the previous fragment, the partial certificate is
discarded on the assumption that the expected fragment was lost.

Verification of \RT\ certificates is the most computationally expensive component of the system
as I discuss in \autoref{section-sprocket-cpu-performance}. Thus, it is important to minimize
the amount of effort spent on verification. To this end, a 16-bit Fletcher checksum is appended
to each certificate to ensure integrity over unreliable channels. Also, nodes maintain a
database of certificate checksums, to quickly check whether a certificate has already been
received and verified. Fletcher checksums are commonly used in sensor networks and other
embedded systems since their error detection properties are almost as good as CRCs with
significantly reduced computational cost \cite{fletcher-1982}.

Currently certificates carry no lifetime information and are considered to be valid forever.
This is not ideal since a certificate issuer may eventually change her policy but currently has
no way to revoke old certificates. However, adding a feature for certificate revocation
introduces non-monotonicity into the semantics of the authorization logic
\cite{Li01nonmonotonicity,Rivest:1998:WEC:647502.728327}. Adding an expiration time to the
certificates is more logically appealing but would require nodes to support real time services
and some degree of time synchronization. This is a non-trivial extension of the basic system
that was beyond the scope of my work.

\subsubsection{Session Key Negotiation}

Public key cryptography is much too computationally expensive to use for authorizing routine
duty postings. Sprocket's run time system addresses this by negotiating session keys between the
client and server nodes. \autoref{figure-sessionkey-daemon} shows the session key processing
architecture of a node.

\begin{figure}[htbp]
  \input{Figures/SessionKey-Daemon}
  \centerline{\raise 1em\box\graph}
  \caption{Session Key Processing Architecture}
  \label{figure-sessionkey-daemon}
\end{figure}

The client maintains a session key storage that is indexed by the triple $(N, C, I)$ where $N$
is the remote node ID, $C$ is the remote component ID, and $I$ is the remote interface ID. A
session key is thus created for each combination of these IDs. The server also maintains a
session key storage indexed by $(N, C, I)$. In this case $N$ is the node ID of the client and
$C$, $I$ are the component and interface IDs on the server to which that client is
communicating. Since any given node can be both server and client, each session key storage
entry has a flag to indicate the nature (client-side or server-side) of the session key.

% Because the session key storage is indexed by node ID and not by the requesting entity's key,
% this causes a (minor?) anomaly: If a client invokes a service ``as A'' and then later tries to
% invoke that same service ``as B'' the second invocation will use the session key created for
% the first. This is only a problem if A has access to the service but B does not. Note that
% it's not a problem for the server: the node is in domain A, with access, regardless. It's only
% a problem for a client that is trying to ``dumb down'' a particular request; access might
% succeed unintentionally.
%
% Fixing this would require storing some kind of key identifier (the whole key?) in the session
% key storage entry. Then duty post messages would need to include this identifier as well. This
% has non-trivial implications for memory and network overhead.

The first time a client attempts to access a service on a particular server, it will send a
session key negotiation request. When a server receives a session key negotiation request
message from a client node $N$ containing the public key $K_{cp}$ of the requesting entity (as
mentioned in the !as! clause of the dynamic wire) and the $(C, I)$ address of the desired
service, the following steps are taken:

\begin{enumerate}
\item Authorization of $K_{cp}$ for service $(C, I)$ is checked using the \RT\ minimum model
  computed by the certificate receiver. If authorization fails nothing more is done.
\item A session key is computed using elliptic curve Diffie-Hellman and added to the session key
  storage under the proper $(N, C, I)$ value. The key is stored as a remote client key.
\item A message is returned to the client containing the server's public key $K_{sp}$ and the
  original $(N, C, I)$ values used by the client. This is so the client is able to compute the
  same session key and associate it with the proper endpoint from its perspective.
\end{enumerate}

Note that under this protocol every session key computed between nodes $N_A$ and $N_B$ for the
same requesting entity will be the same. This is not a problem since the server node uses $(C,
I)$ to look up the session key in its storage. If a node attempts to access an unauthorized
service, no entry for that $(C, I)$ will exist in the server's session key storage and access
will fail.

\subsubsection{Authorized RPC Invocations}

Authorized RPC invocations are made using message authentication codes (MACs) on invocation
messages, created with AES session keys. Verification of a MAC for a particular service on the
receiver side constitutes authorization, since a session key for a particular client and service
is negotiated only after client credentials have been collected and verified that establish the
appropriate authorization for the service. \autoref{figure-post} shows the format of authorized
invocation request messages.

\begin{figure}[t]
  \input{Figures/Post-Format}
  \centerline{\raise 1em\box\graph}
  \caption{Duty Post Message}
  \label{figure-post}
\end{figure}

Since invocation of an RPC service on multiple hosts can be made at once in a fan-out wiring
(see \autoref{section-dynamic-wires}), a single invocation request message may apply to multiple
servers in the neighborhood of the client. To conserve bandwidth, fan-out invocation messages
include multiple MACs, since separate session keys are negotiated with each of $n$ servers,
allowing a single message to invoke the same service on the servers. If the duty arguments
consume $d$ bytes of data, then invocation messages consume $2 + n + d + 4n$ bytes. In practice
this puts significant restrictions on the amount of data that can be passed to duties.

As I describe above my implementation uses a 43~byte message payload for sending certificate
fragments. My experience suggests that using the same payload size for invocation messages
allows for reasonable values of both $d$ and $n$.

Alternatively an implementation could send multiple invocation messages with one for each
server, reducing the number of MACs required on each message to one. However, that greatly
increases radio traffic since the duty arguments and active message overhead must be duplicated
for each message.

To conserve space in the invocation messages only a 32~bit MAC is used. Such a small MAC would
not normally be considered secure. However, wireless sensor networks generate data so slowly
that attacking even such a short MAC is not considered feasible
\cite{karlog-tinysec-2004,luk-minisec-2007}. However, in other environments a larger MAC may be
necessary further increasing message size.


\subsubsection{Security Properties}
\label{section-security-properties}

I stress that this scheme is intended to enforce authorization, which is achieved via the
protocols described above. Integrity is a side effect of this, since MACs are used to enforce
authorization, which are computed over complete message payloads and are verified by the
receiver. Although confidentiality is not directly supported by the current system, it could be
easily added. In particular payloads could be encrypted using negotiated session keys (payloads
are currently sent as plaintext).

\Sprocket\ does not provide any form of replay protection out of the box, but this can be added
at the application level. For example an application could pass a counter as an additional duty
argument. The server could verify that the count increases monotonically as a simple form of
replay protection. Delegating replay protection to the application is appropriate since
SpartanRPC is intended to be a low level infrastructure on which more complex systems can be
built. Furthermore the need for replay protection is likely to be application specific.

Perhaps the most problematic vulnerability of this system is to denial of service attacks. It is
not clear how these could be mitigated without significant changes to the underlying security
protocols. For example, a constant flood of certificates over the correct active message channel
would place receiving nodes in a constant state of ECC digital signature verification,
potentially consuming large amounts of CPU time and energy. Mitigation of such attacks is
outside the scope of my work but has been discussed in the literature \cite{4431860}.

\paragraph{A note on multicast security} Fan-out wirings are a common idiom, and provide a form
of multicast communication. However, the use of MACs for security in a multicast setting
presents well-known challenges. In particular, while $n$-way Diffie-Hellman can be used to
negotiate secret keys between $n$ actors, such a scheme cannot be used in light of the possibly
heterogeneous authorization requirements I anticipate. For example, suppose a node $A$ fan-out
wires to service $s$ on distinct nodes $B$ and $C$, and suppose also that $A$ is authorized for
$s$ on both nodes but that $B$ is not authorized for $s$ on $C$ and vice-versa. If a single
session key were negotiated between $A$, $B$, and $C$ in this case, then $B$ could make
unauthorized use of $C$'s version of $s$ and vice-versa. While a variety of techniques have been
proposed to mitigate this problem \cite{canetti-1999}, most typically rely on very large
multicast groups and are not applicable in my setting. Thus, I handle fan-out wirings using
multiple, independent MACs as described above.

\subsection{Identifying Services Over the Air}

RPC service endpoints are identified by the 4-tuple $(N, C, I, D)$ where $N$ is the TinyOS ID of
the node on which a duty is implemented. $C$ is the local component ID assigned to each
component that provides a remotable interface. $I$ is an interface ID, required since a
component may provide more than one remotable interface. Interface IDs are component-level
unique. Finally $D$ is a duty ID, which must be interface-level unique.

In the current version of \Sprocket, $(C, I)$ values are assigned statically by an arbitrary
(automated or social) process. \Sprocket\ accepts configuration files that define the
association between $(C, I)$ values and the entities to which they refer. Duties are numbered in
the order in which they appear in their enclosing interface definitions.

Some RPC systems, such as ONC RPC, allow each node to provide a registry of RPC services
available on that node \cite{RFC-1833}. When a large number of RPC services are provided by a
node it becomes unreasonable to expect clients to have hard coded knowledge of the endpoint
identifiers for all those services. Instead clients communicate with the single well known
registry to obtain endpoint identifiers that were dynamically assigned. In contrast I assume
this configuration information is known a priori to all interacting actors. It is unclear how
many embedded systems could benefit from a more sophisticated technique for defining and
communicating endpoint identifiers, but it would be an interesting topic for future work.

\subsection{Rewriting SpartanRPC to nesC}

There are five major features requiring SpartanRPC-to-nesC rewriting by \Sprocket: interface
definitions, call sites where remote services are invoked, duty definitions, dynamic wires, and
server components providing remote interfaces. In addition \Sprocket\ generates a stub component
for each dynamic wire, and a skeleton component for each remote interface. Finally \Sprocket\
generates configurations that wrap server components. Here I summarize rewriting strategies for
these features.

% The implementation doesn't actually wrap server components yet. Instead wiring to skeletons is
% done manually after Sprocket runs. Generating the wrapping components should be easy. Globally
% changing existing configurations to use the wrapping components will require some refactoring
% of Sprocket and would be more involved.

\subsubsection{Interfaces, Call Sites, and Duty Definitions}

Duty declarations in interfaces are rewritten to command declarations by substituting !command!
for !duty!. Since nesC commands are allowed to have arbitrary parameters, duties with parameters
present no complications. \Sprocket\ verifies that if an interface contains a duty, then the
only declarations in that interface are duties. \Sprocket\ further verifies that the parameters
of each duty, if any, conform to the restrictions described in \autoref{section-remotable}.

% Some of these checks are not yet implemented, but they should present no problems. The current
% implementation only supports duties with parameters of primitive types (no structure types).

Call sites where duties are posted are rewritten to command invocations by substituting !call!
for !post!. Only post operations applied to duties are rewritten in this way. Finally, duty
definitions are rewritten to command definitions by also substituting !command! for !duty!.

\subsubsection{Authorization Interfaces}

The rewriting process makes use of two interfaces that mediate the interaction between the
\Sprocket\ generated code and the security processing components of the run-time system.
\autoref{figure-client-server-authorization} shows how a message, entering from the left, is
extended with authorization information by the client and then passed to the server where the
authorization information is checked.

\begin{figure}[htbp]
  \input{Figures/Authorization-Interfaces}
  \centerline{\raise 1em\box\graph}
  \caption{Client/Server Authorization Architecture}
  \label{figure-client-server-authorization}
\end{figure}

The \texttt{AuthorizationClient} interface abstracts the details of how an authorized message is
prepared before being sent. The \texttt{AuthorizationServer} interface abstracts the details of
how authorized messages are processed after they are received. This design allows for pluggable
authorization mechanisms. Future versions of \Sprocket\ could support other authorization
schemes than those described here, in a modular fashion.

The authorization interfaces provide their services in a split-phase manner so that potentially
long-running authorization computations can be performed while allowing the node to continue
other functions. In the current implementation, two kinds of authorization are supported. On the
client side the precise method used depends on the dynamic wire over which a particular
communication takes place. On the server side it depends on the presence of a !requires! clause
on the remotely provided interface.

The full $RT_0$ mechanism is supported by client and server components !ACRT0C! and !ASRT0C!
respectively. In addition a ``null'' authorization is supported by client and server components
!ACNullC! and !ASNullC! respectively. The null authorization components perform no operation.
They are used for dynamic wires that do not require authorization and remote interfaces provided
publicly by servers.

\subsubsection{Dynamic Wires}

In the following, I use italics for metavariables that range over arbitrary identifiers. The
reader is referred to the rewriting schema defined in \autoref{figure-dynamic-wire-rewriting}.
Configurations containing dynamic wires are rewritten to configurations that statically wire the
using component \code{\textit{ClientC}} to a stub \code{Spkt\_\textit{n}} that interacts with
the appropriate component manager \code{\textit{SelectorC}} and that handles the communication
channel. Every stub generated by \Sprocket\ is uniquely identified over the scope of the entire
program by an arbitrary integer $n$. The \code{\textit{AuthorizerC}} component is !ACNullC! in
the case where no authorization is requested.

\begin{fpfig}[t]{Dynamic Wire Rewriting}{figure-dynamic-wire-rewriting}
{
\singlespace
\begin{lstlisting}[escapechar=@]
@\textrm{\textit{Dynamic Wire}}@
    @\textit{ClientC}@.@\textit{I}@ -> [@\textit{SelectorC}@].@\textit{I}@;

@\textrm{\textit{Rewritten as\ldots}}@
    components Spkt_@\textit{n}@;
    @\textit{ClientC}@.@\textit{I}@ -> Spkt_@\textit{n}@;
    Spkt_@\textit{n}@.ComponentManager -> @\textit{SelectorC}@;
    Spkt_@\textit{n}@.AuthorizationClient -> @\textit{AuthorizerC}@;
    Spkt_@\textit{n}@.Packet -> AMSenderC;
    Spkt_@\textit{n}@.AMSend -> AMSenderC;
\end{lstlisting}
\primaryspacing
}
\end{fpfig}

In contrast a dynamic wire using either an !enable! or !as! clause is rewritten the same way
except that the \code{\textit{AuthorizerC}} component is !ACRT0C!. Furthermore, the list of
enabled credentials is added to local certificate storage by \Sprocket. Certificates in storage
are periodically beaconed at run-time as described above. Finally, the entity on whose behalf
the RPC invocation is performed is specified in the session key negotiation message sent to the
server, also as described above.

The \code{Spkt\_\textit{n}} stub provides the same interface provided by
\code{\textit{ClientC}}. Wherever a duty is posted by \code{\textit{ClientC}} in source code,
the rewritten call invokes code in the stub that was specialized to handle that duty. The stub
calls into the component manager at run time to obtain a list of the dynamic wire's endpoints
and then prepares a data packet containing remote endpoint addresses and marshaled duty
arguments. Finally the stub calls through the !AuthorizationClient! interface to perform
whatever authorization is needed.

\subsubsection{Remote Services}

For nodes supporting RPC services, \Sprocket\ generates a skeleton component for each remote
interface provided. \autoref{figure-server-skeleton-generation} shows the form of a generated
skeleton for an interface $I$ providing a single duty $d$ that takes a single integer parameter.
This is for purposes of illustration; the scheme is generalized in an obvious manner. In
general, the skeleton contains a task corresponding to each duty provided in the interface, and
every generated skeleton is distinguished by a unique integer $n$ taken from the same numbering
space as the generated stubs.

\begin{fpfig}[t]{Server Skeleton Generation}{figure-server-skeleton-generation}
{
\singlespace
\begin{lstlisting}[escapechar=@]
@\textrm{\textit{Server Component}}@
  module @\textit{ServerC}@ {
    provides remote interface @\textit{I}@ requires @\textit{"A.g"}@;
    @\textit{other (local) uses/provides}@
  }

@\textrm{\textit{Skeleton generated as\ldots}}@
  module Spkt_@\textit{n}@ {
    uses interface @\textit{I}@;
    uses interface Receive;
    uses interface AuthorizationServer;
  }
  implementation {

    @\textit{int value\_1;}@
    @\textit{task void d() \{}@
      @\textit{call I.d(value\_1);}@
    @\textit{\}}@

    event message_t *Receive.receive( ... ) {
      ...
    }
  }
\end{lstlisting}
\primaryspacing
}
\end{fpfig}

When messages are received on a node that provides RPC services, they are examined to see if
they are duty postings and thus to be handled by a skeleton. If so, the !AuthorizationServer!
interface is used to authorize the message. If authorization succeeds, the task corresponding to
the specified duty is posted. That task simply calls into \code{\textit{ServerC}} through the
original interface \code{\textit{I}}. Thus the task-like behavior of duties is ultimately
implemented using actual nesC tasks inside the server skeletons. Duty parameters are conveyed
via module-level variables accessed by the duty tasks (since nesC tasks do not take formal
arguments).

For each component that provides at least one remote interface, \Sprocket\ creates a
configuration as shown in \autoref{figure-server-skeleton-wiring} that wires the corresponding
skeleton(s) to that component. This new configuration wraps the original component and replaces
uses of the original component in other configurations in the program.

In this Figure, as is the case for client-side code, the \code{\textit{AuthorizerC}} component
is either !ASRT0C! or !ASNullC! depending on whether the original remote interface specified
authorization or not.

% This is the part that isn't implemented yet. Right now skeleton wiring is done manually.

\begin{fpfig}[t]{Server Skeleton Wiring}{figure-server-skeleton-wiring}
{
\singlespace
\begin{lstlisting}[escapechar=@]
configuration @\textit{ServerC}@_SpktC {
    @\textit{other (local) uses/provides}@
}
implementation {
    components @\textit{ServerC}@, Spkt_@\textit{n}@;
    Spkt_@\textit{n}@.@\textit{I}@ -> @\textit{ServerC}@;
    Spkt_@\textit{n}@.Receive -> AMReceiverC;
    Spkt_@\textit{n}@.AuthorizationServer -> @\textit{AuthorizerC}@;
    @\textit{pass local uses/provides directly to ServerC}@
}
\end{lstlisting}
\primaryspacing
}
\end{fpfig}

\lstDeleteShortInline!

%%% Local Variables: 
%%% mode: LaTeX
%%% TeX-master: "main"
%%% End: 
